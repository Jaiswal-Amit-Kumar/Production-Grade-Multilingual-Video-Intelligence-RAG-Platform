{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2a3e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and model setup\n",
    "from typing import List, Any, Dict, Annotated, TypedDict, Optional\n",
    "from operator import add\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "import os, json, time\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f721be5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gauta\\.conda\\envs\\langgraph_projects\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# external libs (these do the real work inside nodes)\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from pytube import Playlist\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END, add_messages, MessagesState\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f816e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM client (adjust if your Ollama endpoint differs)\n",
    "llm = ChatOllama(model=\"llama3.1:latest\", base_url=\"http://localhost:11434\", reasoning=False, streaming=False, request_timeout=600.0)\n",
    "# Cross-encoder for rerank\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c37e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages you care about\n",
    "COMMON_LANGUAGES = ['en', 'hi', 'es', 'zh-Hans', 'ar', 'fr', 'ru', 'pt', 'bn', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae94fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß† Cell 2 ‚Äî State schema\n",
    "# ==========================================================\n",
    "class VideoRAGState(TypedDict):\n",
    "    video_url: str\n",
    "    query: str\n",
    "    video_id: str\n",
    "    transcripts: Dict[str, List[Dict[str, Any]]]\n",
    "    docs_with_meta: List[Dict[str, Any]]\n",
    "    chunks: List[Any]\n",
    "    unique_chunks: List[Any]\n",
    "    vector_store: Optional[Any]\n",
    "    retriever: Optional[Any]\n",
    "    retrieved_docs: List[Any]\n",
    "    reranked_docs: List[Any]\n",
    "    context_text: str\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05bbe9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üßæ Utility: more robust YouTube URL cleaning (handles youtu.be)\n",
    "# ==========================================================\n",
    "def clean_video_url(video_url: str):\n",
    "    \"\"\"\n",
    "    Returns (cleaned_url, video_id).\n",
    "    Handles both standard and short YouTube URLs.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(video_url)\n",
    "    qs = parse_qs(parsed.query)\n",
    "\n",
    "    # Standard watch?v=\n",
    "    if 'v' in qs and qs['v']:\n",
    "        vid = qs['v'][0]\n",
    "        cleaned = parsed._replace(query=f\"v={vid}\")\n",
    "        return urlunparse(cleaned), vid\n",
    "\n",
    "    # Short youtu.be/<id>\n",
    "    if parsed.netloc.endswith(\"youtu.be\"):\n",
    "        vid = parsed.path.lstrip(\"/\")\n",
    "        if vid:\n",
    "            cleaned = parsed._replace(query=f\"v={vid}\")\n",
    "            return urlunparse(cleaned), vid\n",
    "\n",
    "    raise ValueError(\"Invalid YouTube URL: could not extract video ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a1a96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üßæ Cell 3: transcript fetcher node (fixed)\n",
    "# ==========================================================\n",
    "def fetch_multilingual_transcripts_node(state: VideoRAGState):\n",
    "    clean_url, vid = clean_video_url(state[\"video_url\"])\n",
    "    transcripts_data = {}\n",
    "\n",
    "    os.makedirs(\"transcripts\", exist_ok=True)\n",
    "    for lang in COMMON_LANGUAGES:\n",
    "        fpath = f\"transcripts/{vid}_{lang}.json\"\n",
    "        if os.path.exists(fpath):\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                transcripts_data[lang] = json.load(f)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Fixed: Use the correct API method\n",
    "            api = YouTubeTranscriptApi()\n",
    "            transcript_list = api.fetch(vid)\n",
    "            try:\n",
    "                transcript = transcript_list.find_transcript([lang])\n",
    "                translated_transcript = transcript.translate(lang)\n",
    "                tlist = translated_transcript.fetch()\n",
    "            except:\n",
    "                # Try to get any available transcript and translate\n",
    "                try:\n",
    "                    transcript = transcript_list.find_transcript(['en'])\n",
    "                    translated_transcript = transcript.translate(lang)\n",
    "                    tlist = translated_transcript.fetch()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            structured = [\n",
    "                {\"start\": s[\"start\"], \"duration\": s.get(\"duration\", 0), \"text\": s[\"text\"].strip()}\n",
    "                for s in tlist\n",
    "            ]\n",
    "            transcripts_data[lang] = structured\n",
    "            with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(structured, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {lang} transcript:\", e)\n",
    "            continue\n",
    "\n",
    "    print(\"Fetched languages:\", list(transcripts_data.keys()))\n",
    "    return {\"video_id\": vid, \"transcripts\": transcripts_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b89c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_transcript_quality(state: VideoRAGState):\n",
    "    \"\"\"Debug function to check transcript quality\"\"\"\n",
    "    print(\"\\nüîç DEBUG TRANSCRIPT QUALITY:\")\n",
    "    total_snippets = 0\n",
    "    total_chars = 0\n",
    "    \n",
    "    for lang, snippets in state[\"transcripts\"].items():\n",
    "        print(f\"  {lang}: {len(snippets)} snippets\")\n",
    "        total_snippets += len(snippets)\n",
    "        for snippet in snippets[:3]:  # Show first 3 snippets\n",
    "            text = snippet[\"text\"].strip()\n",
    "            total_chars += len(text)\n",
    "            print(f\"    - '{text[:50]}{'...' if len(text) > 50 else ''}'\")\n",
    "    \n",
    "    print(f\"üìä Total: {total_snippets} snippets, ~{total_chars} characters\")\n",
    "    \n",
    "    # Check if we have substantial content\n",
    "    if total_chars < 500:\n",
    "        print(\"‚ùå WARNING: Very little transcript content available!\")\n",
    "    elif total_chars < 2000:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Limited transcript content available\")\n",
    "\n",
    "# Call this after fetch_multilingual_transcripts_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89d7fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß© Cell 4 ‚Äî Build docs with metadata (FIXED: Proper timestamp calculation)\n",
    "# ==========================================================\n",
    "def build_docs_with_meta_node(state: VideoRAGState):\n",
    "    docs = []\n",
    "    for lang, snippets in state[\"transcripts\"].items():\n",
    "        for i, sn in enumerate(snippets):\n",
    "            # Calculate end_time properly - use next snippet's start time if available\n",
    "            if i + 1 < len(snippets):\n",
    "                end_time = snippets[i + 1][\"start\"]\n",
    "            else:\n",
    "                end_time = sn[\"start\"] + max(sn.get(\"duration\", 10), 10)  # min 10 sec\n",
    "            \n",
    "            meta = {\n",
    "                \"video_id\": state[\"video_id\"],\n",
    "                \"language\": lang,\n",
    "                \"start_time\": sn[\"start\"],\n",
    "                \"end_time\": end_time,\n",
    "                \"text_length\": len(sn[\"text\"].strip())\n",
    "            }\n",
    "            # Only include meaningful content (not empty or too short)\n",
    "            if sn[\"text\"].strip() and len(sn[\"text\"].strip()) > 5:\n",
    "                docs.append({\"text\": sn[\"text\"].strip(), \"metadata\": meta})\n",
    "    \n",
    "    print(f\"‚úÖ Prepared {len(docs)} docs_with_meta (filtered from {sum(len(s) for s in state['transcripts'].values())} total snippets)\")\n",
    "    return {\"docs_with_meta\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a08edd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ‚úÇÔ∏è Cell 5 ‚Äî Split into chunks (IMPROVED: Better chunking strategy)\n",
    "# ==========================================================\n",
    "def split_into_chunks_node(state: VideoRAGState):\n",
    "    # Use larger chunks for better context\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,  # Increased from 500\n",
    "        chunk_overlap=150,  # Increased from 100\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \"‡•§\", \"ÿü\", \"„ÄÇ\", \"ÔºÅ\", \"Ôºü\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for doc in state[\"docs_with_meta\"]:\n",
    "        if len(doc[\"text\"]) > 50:  # Only split substantial text\n",
    "            doc_obj = Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"])\n",
    "            split_docs = splitter.split_documents([doc_obj])\n",
    "            all_chunks.extend(split_docs)\n",
    "        else:\n",
    "            # Keep short docs as-is\n",
    "            all_chunks.append(Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"]))\n",
    "\n",
    "    # Better deduplication\n",
    "    seen_content = set()\n",
    "    unique_chunks = []\n",
    "    \n",
    "    for chunk in all_chunks:\n",
    "        # Normalize text for deduplication\n",
    "        normalized = ' '.join(chunk.page_content.strip().split())\n",
    "        if normalized and len(normalized) > 10 and normalized not in seen_content:\n",
    "            seen_content.add(normalized)\n",
    "            unique_chunks.append(chunk)\n",
    "\n",
    "    # Convert to dict format for state\n",
    "    chunk_dicts = [\n",
    "        {\"text\": c.page_content, \"metadata\": c.metadata}\n",
    "        for c in all_chunks\n",
    "    ]\n",
    "    unique_dicts = [\n",
    "        {\"text\": c.page_content, \"metadata\": c.metadata}\n",
    "        for c in unique_chunks\n",
    "    ]\n",
    "\n",
    "    print(f\"‚úÖ Chunks: {len(chunk_dicts)}, Unique: {len(unique_dicts)}\")\n",
    "    return {\"chunks\": chunk_dicts, \"unique_chunks\": unique_dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10e25e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß† Cell 6 ‚Äî Build or load vector store (FIXED: Proper Chroma usage)\n",
    "# ==========================================================\n",
    "def build_vector_store_node(state: VideoRAGState):\n",
    "    vid = state[\"video_id\"]\n",
    "    db_dir = f\"chroma_db/{vid}\"\n",
    "    os.makedirs(\"chroma_db\", exist_ok=True)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "        model_kwargs={\"trust_remote_code\": True},\n",
    "    )\n",
    "\n",
    "    texts = [c[\"text\"] for c in state[\"unique_chunks\"]]\n",
    "    metadatas = [c[\"metadata\"] for c in state[\"unique_chunks\"]]\n",
    "\n",
    "    if os.path.exists(db_dir) and os.listdir(db_dir):\n",
    "        print(f\"‚úÖ Using cached vector store for {vid}\")\n",
    "        vectordb = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(f\"üöÄ Creating new vector store for {vid}\")\n",
    "        vectordb = Chroma.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            persist_directory=db_dir,\n",
    "        )\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    return {\"vector_store\": vectordb, \"retriever\": retriever}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99d25074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_retrieval(state: VideoRAGState):\n",
    "    \"\"\"Debug retrieval to see what's actually being found\"\"\"\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "        model_kwargs={\"trust_remote_code\": True},\n",
    "    )\n",
    "    \n",
    "    vectordb = Chroma(\n",
    "        persist_directory=f\"chroma_db/{state['video_id']}\", \n",
    "        embedding_function=embeddings\n",
    "    )\n",
    "    \n",
    "    # Use the same retriever configuration as in retrieve_node\n",
    "    retriever = vectordb.as_retriever(\n",
    "        search_type=\"mmr\",\n",
    "        search_kwargs={\n",
    "            \"k\": 20,\n",
    "            \"fetch_k\": 20,\n",
    "            \"lambda_mult\": 0.7\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    test_docs = retriever.invoke(state[\"query\"])\n",
    "    print(f\"\\nüîç DEBUG RETRIEVAL for query: '{state['query']}'\")\n",
    "    for i, doc in enumerate(test_docs):\n",
    "        print(f\"  Doc {i+1}: '{doc.page_content[:100]}...'\")\n",
    "    \n",
    "    return test_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e12e741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üîç Cell 7 ‚Äî Retrieve (IMPROVED: Better search parameters)\n",
    "# ==========================================================\n",
    "def retrieve_node(state: VideoRAGState):\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            model_kwargs={\"trust_remote_code\": True},\n",
    "        )\n",
    "        \n",
    "        # Load vector store\n",
    "        vectordb = Chroma(\n",
    "            persist_directory=f\"chroma_db/{state['video_id']}\", \n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # Try simpler retrieval first\n",
    "        retriever = vectordb.as_retriever(\n",
    "            search_type=\"similarity\",  # Use simple similarity first\n",
    "            search_kwargs={\n",
    "                \"k\": 20,  # Get more documents\n",
    "            }\n",
    "        )\n",
    "\n",
    "        docs = retriever.invoke(state[\"query\"])\n",
    "        print(f\"‚úÖ Retrieved {len(docs)} documents\")\n",
    "        \n",
    "        # Filter out very short documents but be less strict\n",
    "        filtered_docs = []\n",
    "        for doc in docs:\n",
    "            content = doc.page_content.strip()\n",
    "            if len(content) > 10:  # Reduced from 20\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        print(f\"‚úÖ Filtered to {len(filtered_docs)} documents\")\n",
    "        \n",
    "        # Convert to dict format\n",
    "        docs_dict = [{\"text\": d.page_content, \"metadata\": d.metadata} for d in filtered_docs]\n",
    "        return {\"retrieved_docs\": docs_dict}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in retrieval: {e}\")\n",
    "        return {\"retrieved_docs\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2137c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìä Cell 8 ‚Äî Rerank (FIXED: Proper threshold for cross-encoder)\n",
    "# ==========================================================\n",
    "def rerank_node(state: VideoRAGState, top_n=7):\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    if not docs:\n",
    "        print(\"‚ùå No documents to rerank\")\n",
    "        return {\"reranked_docs\": []}\n",
    "\n",
    "    # Create query-document pairs\n",
    "    pairs = [[state[\"query\"], d[\"text\"]] for d in docs]\n",
    "    \n",
    "    # Get scores from cross-encoder\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    \n",
    "    # For this cross-encoder model, scores can be negative\n",
    "    # Use a more appropriate threshold or just take top N regardless\n",
    "    scored_docs = list(zip(scores, docs))\n",
    "    \n",
    "    # Sort by score (highest first)\n",
    "    scored_docs.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    # Take top N without threshold, or use a very low threshold\n",
    "    top_docs = [doc for score, doc in scored_docs[:top_n]]\n",
    "    \n",
    "    print(f\"‚úÖ Reranked: {len(top_docs)} docs (top {top_n})\")\n",
    "    print(f\"üìä Score range: {min(scores):.3f} - {max(scores):.3f}\")\n",
    "    print(f\"üìà Top score: {scored_docs[0][0]:.3f}\" if scored_docs else \"No scores\")\n",
    "    \n",
    "    return {\"reranked_docs\": top_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e86df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß± Cell 9 ‚Äî Format context (FIXED: Handle low-score results)\n",
    "# ==========================================================\n",
    "def format_context_node(state: VideoRAGState):\n",
    "    if not state[\"reranked_docs\"]:\n",
    "        print(\"‚ö†Ô∏è No high-scoring docs, but let's use what we have\")\n",
    "        # If reranking filtered everything out, use the original retrieved docs\n",
    "        if \"retrieved_docs\" in state and state[\"retrieved_docs\"]:\n",
    "            print(\"üîÑ Falling back to original retrieved docs\")\n",
    "            docs_to_use = state[\"retrieved_docs\"][:5]  # Use top 5 original docs\n",
    "        else:\n",
    "            return {\"context_text\": \"No relevant information found in the video.\"}\n",
    "\n",
    "    else:\n",
    "        docs_to_use = state[\"reranked_docs\"]\n",
    "\n",
    "    def seconds_to_timestamp(seconds):\n",
    "        \"\"\"Convert seconds to MM:SS format\"\"\"\n",
    "        minutes = int(seconds // 60)\n",
    "        secs = int(seconds % 60)\n",
    "        return f\"{minutes:02d}:{secs:02d}\"\n",
    "\n",
    "    context_parts = []\n",
    "    for i, d in enumerate(docs_to_use):\n",
    "        start_time = d['metadata'].get('start_time', 0)\n",
    "        end_time = d['metadata'].get('end_time', start_time + 10)\n",
    "        \n",
    "        time_range = f\"{seconds_to_timestamp(start_time)}-{seconds_to_timestamp(end_time)}\"\n",
    "        context_parts.append(f\"Segment {i+1} [{time_range}]: {d['text']}\")\n",
    "    \n",
    "    context_text = \"\\n\\n\".join(context_parts)\n",
    "    print(f\"‚úÖ Formatted context with {len(context_parts)} segments\")\n",
    "    return {\"context_text\": context_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31bcf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìù Cell 10 ‚Äî Summarize full video (IMPROVED: Better content handling)\n",
    "# ==========================================================\n",
    "def summarize_video_node(state: VideoRAGState, max_chars=25000):\n",
    "    if not state[\"unique_chunks\"]:\n",
    "        return {\"answer\": \"No content available for summarization.\"}\n",
    "    \n",
    "    # Use all unique chunks for summary\n",
    "    all_text = \" \".join([c[\"text\"] for c in state[\"unique_chunks\"]])[:max_chars]\n",
    "    \n",
    "    if len(all_text.strip()) < 100:\n",
    "        return {\"answer\": \"Not enough transcript content available for a meaningful summary.\"}\n",
    "    \n",
    "    prompt_text = f\"\"\"\n",
    "You are an expert video summarizer. Create a comprehensive summary of the following YouTube transcript.\n",
    "\n",
    "TRANSCRIPT CONTENT:\n",
    "{all_text}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Create a detailed, well-structured summary\n",
    "- Capture the main themes and key points\n",
    "- Maintain the original meaning and context\n",
    "- Write in clear, natural language\n",
    "\n",
    "SUMMARY:\n",
    "\"\"\"\n",
    "    try:\n",
    "        resp = llm.invoke(prompt_text)\n",
    "        print(\"‚úÖ Full video summary generated\")\n",
    "        return {\"answer\": resp.content}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in summarization: {e}\")\n",
    "        return {\"answer\": \"Error generating summary.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad37f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üí¨ Cell 11 ‚Äî LLM answer node (FIXED: Proper prompt handling)\n",
    "# ==========================================================\n",
    "def llm_answer_node(state: VideoRAGState):\n",
    "    query = state[\"query\"].lower()\n",
    "    \n",
    "    # If we have very little context, be upfront about it\n",
    "    if \"No relevant information\" in state[\"context_text\"] or len(state[\"context_text\"]) < 100:\n",
    "        return {\"answer\": \"I couldn't find specific information in the video that directly answers your question. The transcript content may not cover this topic, or the retrieval didn't find relevant matches.\"}\n",
    "    \n",
    "    # Detect if this is a timing-related question\n",
    "    time_keywords = [\n",
    "        \"when\", \"time\", \"timestamp\", \"at what time\", \"minute\", \"second\", \n",
    "        \"hour\", \"duration\", \"start\", \"end\", \"begin\", \"moment\", \"point in time\",\n",
    "        \"how long\", \"what time\", \"which part\", \"where in the video\", \"at which\"\n",
    "    ]\n",
    "    \n",
    "    include_timestamps = any(keyword in query for keyword in time_keywords)\n",
    "    \n",
    "    if include_timestamps:\n",
    "        prompt_template = \"\"\"Based on the video content below, answer the user's question focusing on timing information:\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Answer with specific timestamps when possible. If exact timing isn't available, provide the best estimate based on the context.\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt_template = \"\"\"Based on the video content below, answer the user's question:\n",
    "\n",
    "CONTEXT:\n",
    "{context_text}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Provide a helpful answer using the available context. If the context doesn't fully address the question, acknowledge this limitation.\n",
    "\"\"\"\n",
    "\n",
    "    # Create prompt inside the function\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"context_text\", \"query\"],\n",
    "        template=prompt_template\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        chain = prompt | llm | StrOutputParser()\n",
    "        resp = chain.invoke({\"context_text\": state[\"context_text\"], \"query\": state[\"query\"]})\n",
    "        print(\"‚úÖ LLM answer generated\")\n",
    "        return {\"answer\": resp}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error in LLM: {e}\")\n",
    "        return {\"answer\": \"I encountered an error while processing your question. Please try again with a different query.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81424e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üéØ Router function to decide summary vs Q&A\n",
    "# ==========================================================\n",
    "def route_question(state: VideoRAGState):\n",
    "    \"\"\"Route to summary or Q&A based on query\"\"\"\n",
    "    query = state[\"query\"].lower().strip()\n",
    "    if query == \"summarize\" or \"summary\" in query:\n",
    "        return \"summarize\"\n",
    "    else:\n",
    "        return \"retrieve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55f0289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üîó Cell 12 ‚Äî Build graph (FIXED: Proper routing)\n",
    "# ==========================================================\n",
    "graph = StateGraph(VideoRAGState)\n",
    "\n",
    "graph.add_node(\"fetch_transcripts\", fetch_multilingual_transcripts_node)\n",
    "graph.add_node(\"build_docs\", build_docs_with_meta_node)\n",
    "graph.add_node(\"split\", split_into_chunks_node)\n",
    "graph.add_node(\"vectorize\", build_vector_store_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"rerank\", rerank_node)\n",
    "graph.add_node(\"format\", format_context_node)\n",
    "graph.add_node(\"answer\", llm_answer_node)\n",
    "graph.add_node(\"summarize\", summarize_video_node)\n",
    "\n",
    "graph.add_edge(START, \"fetch_transcripts\")\n",
    "graph.add_edge(\"fetch_transcripts\", \"build_docs\")\n",
    "graph.add_edge(\"build_docs\", \"split\")\n",
    "graph.add_edge(\"split\", \"vectorize\")\n",
    "\n",
    "# Add conditional routing after vectorize\n",
    "graph.add_conditional_edges(\n",
    "    \"vectorize\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"summarize\": \"summarize\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"rerank\")\n",
    "graph.add_edge(\"rerank\", \"format\")\n",
    "graph.add_edge(\"format\", \"answer\")\n",
    "graph.add_edge(\"summarize\", END)\n",
    "graph.add_edge(\"answer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "print(\"‚úÖ Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32dd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üöÄ Runner (FIXED: Proper state initialization)\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Gather inputs once (avoid double prompt)\n",
    "    video_url = input(\"Enter YouTube URL: \").strip()\n",
    "    query = input(\"Enter your question: \").strip()\n",
    "\n",
    "    # Build initial TypedDict state\n",
    "    state: VideoRAGState = {\n",
    "        \"video_url\": video_url,\n",
    "        \"query\": query,\n",
    "        \"video_id\": \"\",\n",
    "        \"transcripts\": {},\n",
    "        \"docs_with_meta\": [],\n",
    "        \"chunks\": [],\n",
    "        \"unique_chunks\": [],\n",
    "        \"vector_store\": None,\n",
    "        \"retriever\": None,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"reranked_docs\": [],\n",
    "        \"context_text\": \"\",\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        # Run the compiled graph\n",
    "        final_state = app.invoke(state)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Please check the YouTube URL and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425abbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Final Answer:\n",
      " Based on the provided segments of the video, it's difficult to provide a comprehensive summary without seeing more context or content from the full video. However, I can attempt to piece together some information based on the given snippets.\n",
      "\n",
      "The video seems to touch on various themes related to understanding and empathy, including:\n",
      "\n",
      "- The importance of \"opening up your heart\" to understanding others (Segment 3)\n",
      "- Emphasizing that one should understand the other side (Segment 4)\n",
      "- A story or anecdote is mentioned in a lighthearted manner (Segments 7 and possibly Segment 2, suggesting a narrative approach to storytelling), which might be used for illustrative purposes.\n",
      "  \n",
      "Without more content from the video, it's hard to determine what specific topics are covered or how they relate to each other. The themes of understanding, empathy, and perhaps communication strategies through storytelling are hinted at but require more context to fully understand their implications in the video.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ Final Answer:\\n\", final_state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d832bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
