{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f2a3e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: imports and model setup\n",
    "from typing import List, Any, Dict, Annotated, TypedDict, Optional\n",
    "from operator import add\n",
    "from pydantic import BaseModel, HttpUrl, ValidationError\n",
    "import os, json, time\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f721be5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# external libs (these do the real work inside nodes)\n",
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from pytube import Playlist\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "# from langchain_community.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "from sentence_transformers import CrossEncoder\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langgraph.graph import StateGraph, START, END, add_messages, MessagesState\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f816e18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM client (adjust if your Ollama endpoint differs)\n",
    "llm = ChatOllama(model=\"llama3.1:latest\", base_url=\"http://localhost:11434\", reasoning=False, streaming=False, request_timeout=600.0)\n",
    "# Cross-encoder for rerank\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c37e334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# languages you care about\n",
    "COMMON_LANGUAGES = ['en', 'hi', 'es', 'zh-Hans', 'ar', 'fr', 'ru', 'pt', 'bn', 'de']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bae94fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß† Cell 2 ‚Äî State schema\n",
    "# ==========================================================\n",
    "class VideoRAGState(TypedDict):\n",
    "    video_url: str\n",
    "    query: str\n",
    "    video_id: str\n",
    "    transcripts: Dict[str, List[Dict[str, Any]]]\n",
    "    docs_with_meta: List[Dict[str, Any]]\n",
    "    chunks: List[Any]\n",
    "    unique_chunks: List[Any]\n",
    "    vector_store: Optional[Any]\n",
    "    retriever: Optional[Any]\n",
    "    retrieved_docs: List[Any]\n",
    "    reranked_docs: List[Any]\n",
    "    context_text: str\n",
    "    answer: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05bbe9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üßæ Utility: more robust YouTube URL cleaning (handles youtu.be)\n",
    "# ==========================================================\n",
    "def clean_video_url(video_url: str):\n",
    "    \"\"\"\n",
    "    Returns (cleaned_url, video_id).\n",
    "    Handles both standard and short YouTube URLs.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(video_url)\n",
    "    qs = parse_qs(parsed.query)\n",
    "\n",
    "    # Standard watch?v=\n",
    "    if 'v' in qs and qs['v']:\n",
    "        vid = qs['v'][0]\n",
    "        cleaned = parsed._replace(query=f\"v={vid}\")\n",
    "        return urlunparse(cleaned), vid\n",
    "\n",
    "    # Short youtu.be/<id>\n",
    "    if parsed.netloc.endswith(\"youtu.be\"):\n",
    "        vid = parsed.path.lstrip(\"/\")\n",
    "        if vid:\n",
    "            cleaned = parsed._replace(query=f\"v={vid}\")\n",
    "            return urlunparse(cleaned), vid\n",
    "\n",
    "    raise ValueError(\"Invalid YouTube URL: could not extract video ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a1a96d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üßæ Cell 3: transcript fetcher node (fixed)\n",
    "# ==========================================================\n",
    "def fetch_multilingual_transcripts_node(state: VideoRAGState):\n",
    "    clean_url, vid = clean_video_url(state[\"video_url\"])\n",
    "    transcripts_data = {}\n",
    "\n",
    "    os.makedirs(\"transcripts\", exist_ok=True)\n",
    "    for lang in COMMON_LANGUAGES:\n",
    "        fpath = f\"transcripts/{vid}_{lang}.json\"\n",
    "        if os.path.exists(fpath):\n",
    "            with open(fpath, \"r\", encoding=\"utf-8\") as f:\n",
    "                transcripts_data[lang] = json.load(f)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Fixed: Use the correct API method\n",
    "            api = YouTubeTranscriptApi()\n",
    "            transcript_list = api.fetch(vid)\n",
    "            try:\n",
    "                transcript = transcript_list.find_transcript([lang])\n",
    "                translated_transcript = transcript.translate(lang)\n",
    "                tlist = translated_transcript.fetch()\n",
    "            except:\n",
    "                # Try to get any available transcript and translate\n",
    "                try:\n",
    "                    transcript = transcript_list.find_transcript(['en'])\n",
    "                    translated_transcript = transcript.translate(lang)\n",
    "                    tlist = translated_transcript.fetch()\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            structured = [\n",
    "                {\"start\": s[\"start\"], \"duration\": s.get(\"duration\", 0), \"text\": s[\"text\"].strip()}\n",
    "                for s in tlist\n",
    "            ]\n",
    "            transcripts_data[lang] = structured\n",
    "            with open(fpath, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(structured, f, ensure_ascii=False, indent=2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching {lang} transcript:\", e)\n",
    "            continue\n",
    "\n",
    "    print(\"Fetched languages:\", list(transcripts_data.keys()))\n",
    "    return {\"video_id\": vid, \"transcripts\": transcripts_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89d7fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß© Cell 4 ‚Äî Build docs with metadata (fixed)\n",
    "# ==========================================================\n",
    "def build_docs_with_meta_node(state: VideoRAGState):\n",
    "    docs = []\n",
    "    for lang, snippets in state[\"transcripts\"].items():\n",
    "        for sn in snippets:\n",
    "            meta = {\n",
    "                \"video_id\": state[\"video_id\"],\n",
    "                \"language\": lang,\n",
    "                \"start_time\": sn[\"start\"],\n",
    "                \"end_time\": sn[\"start\"] + sn[\"duration\"],\n",
    "            }\n",
    "            docs.append({\"text\": sn[\"text\"].strip(), \"metadata\": meta})\n",
    "    print(\"‚úÖ Prepared docs_with_meta:\", len(docs))\n",
    "    return {\"docs_with_meta\": docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a08edd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# ‚úÇÔ∏è Cell 5 ‚Äî Split into chunks (FIXED: Use Document objects properly)\n",
    "# ==========================================================\n",
    "def split_into_chunks_node(state: VideoRAGState):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, chunk_overlap=100,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \".\", \"‡•§\", \"ÿü\", \"!\", \"„ÄÇ\", \"Ôºå\"]\n",
    "    )\n",
    "    \n",
    "    all_chunks = []\n",
    "    for doc in state[\"docs_with_meta\"]:\n",
    "        # Create Document objects for splitting\n",
    "        doc_obj = Document(page_content=doc[\"text\"], metadata=doc[\"metadata\"])\n",
    "        split_docs = splitter.split_documents([doc_obj])\n",
    "        all_chunks.extend(split_docs)\n",
    "\n",
    "    # Filter + deduplicate\n",
    "    filtered = [c for c in all_chunks if c.page_content.strip()]\n",
    "    seen, unique = set(), []\n",
    "    for c in filtered:\n",
    "        if c.page_content not in seen:\n",
    "            seen.add(c.page_content)\n",
    "            unique.append(c)\n",
    "\n",
    "    # Convert to dict format for state\n",
    "    chunk_dicts = [\n",
    "        {\"text\": c.page_content, \"metadata\": c.metadata}\n",
    "        for c in all_chunks\n",
    "    ]\n",
    "    unique_dicts = [\n",
    "        {\"text\": c.page_content, \"metadata\": c.metadata}\n",
    "        for c in unique\n",
    "    ]\n",
    "\n",
    "    print(f\"‚úÖ Chunks: {len(chunk_dicts)}, Unique: {len(unique_dicts)}\")\n",
    "    return {\"chunks\": chunk_dicts, \"unique_chunks\": unique_dicts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "10e25e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß† Cell 6 ‚Äî Build or load vector store (FIXED: Proper Chroma usage)\n",
    "# ==========================================================\n",
    "def build_vector_store_node(state: VideoRAGState):\n",
    "    vid = state[\"video_id\"]\n",
    "    db_dir = f\"chroma_db/{vid}\"\n",
    "    os.makedirs(\"chroma_db\", exist_ok=True)\n",
    "\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "        model_kwargs={\"trust_remote_code\": True},\n",
    "    )\n",
    "\n",
    "    texts = [c[\"text\"] for c in state[\"unique_chunks\"]]\n",
    "    metadatas = [c[\"metadata\"] for c in state[\"unique_chunks\"]]\n",
    "\n",
    "    if os.path.exists(db_dir) and os.listdir(db_dir):\n",
    "        print(f\"‚úÖ Using cached vector store for {vid}\")\n",
    "        vectordb = Chroma(persist_directory=db_dir, embedding_function=embeddings)\n",
    "    else:\n",
    "        print(f\"üöÄ Creating new vector store for {vid}\")\n",
    "        vectordb = Chroma.from_texts(\n",
    "            texts=texts,\n",
    "            embedding=embeddings,\n",
    "            metadatas=metadatas,\n",
    "            persist_directory=db_dir,\n",
    "        )\n",
    "\n",
    "    retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    return {\"vector_store\": vectordb, \"retriever\": retriever}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e12e741b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üîç Cell 7 ‚Äî Retrieve (FIXED: Use the vector_store from state)\n",
    "# ==========================================================\n",
    "def retrieve_node(state: VideoRAGState):\n",
    "    if \"retriever\" not in state or state[\"retriever\"] is None:\n",
    "        # Fallback: recreate retriever if not in state\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\",\n",
    "            model_kwargs={\"trust_remote_code\": True},\n",
    "        )\n",
    "        vectordb = Chroma(persist_directory=f\"chroma_db/{state['video_id']}\", embedding_function=embeddings)\n",
    "        retriever = vectordb.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})\n",
    "    else:\n",
    "        retriever = state[\"retriever\"]\n",
    "\n",
    "    docs = retriever.invoke(state[\"query\"])\n",
    "    print(\"‚úÖ Retrieved:\", len(docs))\n",
    "\n",
    "    # Convert Document objects ‚Üí dicts\n",
    "    docs_dict = [{\"text\": d.page_content, \"metadata\": d.metadata} for d in docs]\n",
    "    return {\"retrieved_docs\": docs_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2137c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìä Cell 8 ‚Äî Rerank (keeps your cross-encoder usage)\n",
    "# ==========================================================\n",
    "def rerank_node(state: VideoRAGState, top_n=5):\n",
    "    docs = state[\"retrieved_docs\"]\n",
    "    if not docs:\n",
    "        return {\"reranked_docs\": []}\n",
    "\n",
    "    pairs = [[state[\"query\"], d[\"text\"]] for d in docs]\n",
    "    scores = cross_encoder.predict(pairs)\n",
    "    ranked = sorted(zip(scores, docs), key=lambda x: x[0], reverse=True)\n",
    "    top_docs = [d for _, d in ranked[:top_n]]\n",
    "\n",
    "    print(\"‚úÖ Reranked top_n:\", len(top_docs))\n",
    "    return {\"reranked_docs\": top_docs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e86df2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üß± Cell 9 ‚Äî Format context (FIXED: Handle empty case)\n",
    "# ==========================================================\n",
    "def format_context_node(state: VideoRAGState):\n",
    "    if not state[\"reranked_docs\"]:\n",
    "        return {\"context_text\": \"No relevant context found.\"}\n",
    "\n",
    "    context_text = \"\\n\\n\".join(\n",
    "        f\"[{d['metadata'].get('start_time', 0):.2f}s - {d['metadata'].get('end_time', 0):.2f}s] {d['text']}\"\n",
    "        for d in state[\"reranked_docs\"]\n",
    "    )\n",
    "    return {\"context_text\": context_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31bcf4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üìù Cell 10 ‚Äî Summarize full video (FIXED: Use dict access)\n",
    "# ==========================================================\n",
    "def summarize_video_node(state: VideoRAGState, max_chars=30000):\n",
    "    if not state[\"unique_chunks\"]:\n",
    "        return {\"answer\": \"No content available for summarization.\"}\n",
    "    \n",
    "    all_text = \" \".join([c[\"text\"] for c in state[\"unique_chunks\"]])[:max_chars]\n",
    "    prompt_text = f\"\"\"\n",
    "You are an expert video summarizer.\n",
    "Summarize the following YouTube transcript clearly and in detail.\n",
    "\n",
    "Transcript:\n",
    "{all_text}\n",
    "\"\"\"\n",
    "    resp = llm.invoke(prompt_text)\n",
    "    print(\"‚úÖ Full video summary generated.\")\n",
    "    return {\"answer\": resp.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad37f90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üí¨ Cell 11 ‚Äî LLM answer node (FIXED: Proper chain invocation)\n",
    "# ==========================================================\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context_text\", \"query\"],\n",
    "    template=\"Context:\\n{context_text}\\n\\nQuestion: {query}\\n\\nAnswer succinctly with timestamps if possible.\"\n",
    ")\n",
    "\n",
    "def llm_answer_node(state: VideoRAGState):\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    resp = chain.invoke({\"context_text\": state[\"context_text\"], \"query\": state[\"query\"]})\n",
    "    print(\"‚úÖ LLM answer ready.\")\n",
    "    return {\"answer\": resp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81424e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# üéØ Router function to decide summary vs Q&A\n",
    "# ==========================================================\n",
    "def route_question(state: VideoRAGState):\n",
    "    \"\"\"Route to summary or Q&A based on query\"\"\"\n",
    "    query = state[\"query\"].lower().strip()\n",
    "    if query == \"summarize\" or \"summary\" in query:\n",
    "        return \"summarize\"\n",
    "    else:\n",
    "        return \"retrieve\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55f0289d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Graph compiled successfully!\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üîó Cell 12 ‚Äî Build graph (FIXED: Proper routing)\n",
    "# ==========================================================\n",
    "graph = StateGraph(VideoRAGState)\n",
    "\n",
    "graph.add_node(\"fetch_transcripts\", fetch_multilingual_transcripts_node)\n",
    "graph.add_node(\"build_docs\", build_docs_with_meta_node)\n",
    "graph.add_node(\"split\", split_into_chunks_node)\n",
    "graph.add_node(\"vectorize\", build_vector_store_node)\n",
    "graph.add_node(\"retrieve\", retrieve_node)\n",
    "graph.add_node(\"rerank\", rerank_node)\n",
    "graph.add_node(\"format\", format_context_node)\n",
    "graph.add_node(\"answer\", llm_answer_node)\n",
    "graph.add_node(\"summarize\", summarize_video_node)\n",
    "\n",
    "graph.add_edge(START, \"fetch_transcripts\")\n",
    "graph.add_edge(\"fetch_transcripts\", \"build_docs\")\n",
    "graph.add_edge(\"build_docs\", \"split\")\n",
    "graph.add_edge(\"split\", \"vectorize\")\n",
    "\n",
    "# Add conditional routing after vectorize\n",
    "graph.add_conditional_edges(\n",
    "    \"vectorize\",\n",
    "    route_question,\n",
    "    {\n",
    "        \"summarize\": \"summarize\",\n",
    "        \"retrieve\": \"retrieve\",\n",
    "    }\n",
    ")\n",
    "\n",
    "graph.add_edge(\"retrieve\", \"rerank\")\n",
    "graph.add_edge(\"rerank\", \"format\")\n",
    "graph.add_edge(\"format\", \"answer\")\n",
    "graph.add_edge(\"summarize\", END)\n",
    "graph.add_edge(\"answer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "print(\"‚úÖ Graph compiled successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db32dd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched languages: ['en']\n",
      "‚úÖ Prepared docs_with_meta: 1104\n",
      "‚úÖ Chunks: 1104, Unique: 1103\n",
      "‚úÖ Using cached vector store for bdpyQm5l78o\n",
      "‚úÖ Retrieved: 10\n",
      "‚úÖ Reranked top_n: 5\n",
      "‚úÖ LLM answer ready.\n",
      "\n",
      "üéØ Final Answer:\n",
      " The video discusses empathy and understanding by sharing personal stories, aiming to open up one's heart to comprehend others' perspectives. \n",
      "\n",
      "Possible timestamp ranges for key points:\n",
      "- 1253.48s - 1258.84s: Opening hearts to understanding\n",
      "- 2243.72s - 2247.64s: Understanding others\n",
      "- 2231.52s - 2236.08s: Recognizing one's own understanding\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# üöÄ Runner (FIXED: Proper state initialization)\n",
    "# ==========================================================\n",
    "if __name__ == \"__main__\":\n",
    "    # Gather inputs once (avoid double prompt)\n",
    "    video_url = input(\"Enter YouTube URL: \").strip()\n",
    "    query = input(\"Enter your question: \").strip()\n",
    "\n",
    "    # Build initial TypedDict state\n",
    "    state: VideoRAGState = {\n",
    "        \"video_url\": video_url,\n",
    "        \"query\": query,\n",
    "        \"video_id\": \"\",\n",
    "        \"transcripts\": {},\n",
    "        \"docs_with_meta\": [],\n",
    "        \"chunks\": [],\n",
    "        \"unique_chunks\": [],\n",
    "        \"vector_store\": None,\n",
    "        \"retriever\": None,\n",
    "        \"retrieved_docs\": [],\n",
    "        \"reranked_docs\": [],\n",
    "        \"context_text\": \"\",\n",
    "        \"answer\": \"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Run the compiled graph\n",
    "        final_state = app.invoke(state)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Please check the YouTube URL and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "425abbb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ Final Answer:\n",
      " The video discusses empathy and understanding by sharing personal stories, aiming to open up one's heart to comprehend others' perspectives. \n",
      "\n",
      "Possible timestamp ranges for key points:\n",
      "- 1253.48s - 1258.84s: Opening hearts to understanding\n",
      "- 2243.72s - 2247.64s: Understanding others\n",
      "- 2231.52s - 2236.08s: Recognizing one's own understanding\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nüéØ Final Answer:\\n\", final_state[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d832bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
