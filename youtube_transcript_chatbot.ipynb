{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c224f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from sentence_transformers import CrossEncoder\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from urllib.parse import urlparse, parse_qs, urlunparse\n",
    "from pytube import Playlist\n",
    "import time\n",
    "from langdetect import detect, DetectorFactory\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "import re\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9744d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"llama3.1:latest\", base_url=\"http://localhost:11434\", reasoning=False, streaming=False, request_timeout=600.0)\n",
    "\n",
    "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-12-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425b5d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved transcript text: transcripts\\bdpyQm5l78o_en.txt\n",
      "üíæ Saved structured transcript JSON: transcripts\\bdpyQm5l78o_en.json\n",
      "‚úÖ All available transcripts fetched for video: bdpyQm5l78o\n",
      "\n",
      "=== Combined transcripts (simplified text view) ===\n",
      "\n",
      "--- en ---\n",
      "[0.08s] it is important to not sulk now you\n",
      "[2.76s] could call me a monk who did not sell\n",
      "[4.72s] her Ferrari even when you go in an\n",
      "[6.88s] airplane when they're giving that you\n",
      "[8.96s] know emergency uh talk they say when\n",
      "[12.52s] there will be a oxygen mask you put on\n",
      "[14.60s] yourself first and on your child later\n",
      "[17.32s] just say I get to do this right I get to\n",
      "[19.60s] go to work I get to see my friends how\n",
      "[21.96s] much is it changing your life yeah no\n"
     ]
    }
   ],
   "source": [
    "# List of top 10 languages\n",
    "COMMON_LANGUAGES = ['en', 'hi', 'es', 'zh-Hans', 'ar', 'fr', 'ru', 'pt', 'bn', 'de']\n",
    "\n",
    "def clean_video_url(video_url):\n",
    "    \"\"\"Remove playlist parameter and return clean YouTube URL + video ID.\"\"\"\n",
    "    parsed_url = urlparse(video_url)\n",
    "    query_params = parse_qs(parsed_url.query)\n",
    "\n",
    "    if 'v' not in query_params:\n",
    "        raise ValueError(f\"Invalid YouTube video URL: {video_url}\")\n",
    "\n",
    "    clean_query = f\"v={query_params['v'][0]}\"\n",
    "    cleaned_url = urlunparse(parsed_url._replace(query=clean_query))\n",
    "    return cleaned_url, query_params['v'][0]\n",
    "\n",
    "\n",
    "def save_transcript(video_id, lang, transcript_data, output_dir=\"transcripts\"):\n",
    "    \"\"\"\n",
    "    Save transcript data to both text (.txt) and JSON (.json) files.\n",
    "    The JSON file preserves timestamps for later search or analysis.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save plain text version (for readability)\n",
    "    text_path = os.path.join(output_dir, f\"{video_id}_{lang}.txt\")\n",
    "    with open(text_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for snippet in transcript_data:\n",
    "            f.write(f\"[{snippet['start']:.2f}s] {snippet['text']}\\n\")\n",
    "    print(f\"üíæ Saved transcript text: {text_path}\")\n",
    "\n",
    "    # Save structured JSON version\n",
    "    json_path = os.path.join(output_dir, f\"{video_id}_{lang}.json\")\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(transcript_data, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"üíæ Saved structured transcript JSON: {json_path}\")\n",
    "\n",
    "\n",
    "def fetch_multilingual_transcripts(video_url, retries=3, delay=5):\n",
    "    \"\"\"\n",
    "    Fetch transcripts for multiple languages.\n",
    "    Returns a dictionary {language_code: transcript_data_list}.\n",
    "    Also saves each transcript to text and JSON.\n",
    "    \"\"\"\n",
    "    clean_url, video_id = clean_video_url(video_url)\n",
    "    transcripts_data = {}\n",
    "\n",
    "    for lang in COMMON_LANGUAGES:\n",
    "        for attempt in range(1, retries + 1):\n",
    "            try:\n",
    "                api = YouTubeTranscriptApi()\n",
    "                transcript_list = api.fetch(video_id, languages=[lang])\n",
    "\n",
    "                # Convert to a list of structured dictionaries\n",
    "                structured_transcript = [\n",
    "                    {\n",
    "                        \"start\": snippet.start,\n",
    "                        \"duration\": snippet.duration,\n",
    "                        \"text\": snippet.text.strip()\n",
    "                    }\n",
    "                    for snippet in transcript_list\n",
    "                ]\n",
    "\n",
    "                transcripts_data[lang] = structured_transcript\n",
    "                save_transcript(video_id, lang, structured_transcript)\n",
    "                break  # success, move to next language\n",
    "\n",
    "            except TranscriptsDisabled:\n",
    "                print(f\"‚ùå Captions are disabled for video: {clean_url}\")\n",
    "                return None\n",
    "            except NoTranscriptFound:\n",
    "                # Language not available\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Error fetching {clean_url} [{lang}], attempt {attempt}/{retries}: {e}\")\n",
    "                time.sleep(delay)\n",
    "\n",
    "    if transcripts_data:\n",
    "        print(f\"‚úÖ All available transcripts fetched for video: {video_id}\")\n",
    "        return transcripts_data\n",
    "    else:\n",
    "        print(f\"‚ùå No transcripts available for video: {video_id}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- Main Program ---\n",
    "\n",
    "all_transcripts = {}\n",
    "\n",
    "choice = input(\"Do you want to fetch a single video or a playlist? (video/playlist): \").strip().lower()\n",
    "\n",
    "if choice == \"video\":\n",
    "    video_url = input(\"Enter the YouTube video URL: \").strip()\n",
    "    transcripts = fetch_multilingual_transcripts(video_url)\n",
    "    if transcripts:\n",
    "        all_transcripts[video_url] = transcripts\n",
    "        print(\"\\n=== Combined transcripts (simplified text view) ===\")\n",
    "        for lang, data in transcripts.items():\n",
    "            print(f\"\\n--- {lang} ---\")\n",
    "            for snippet in data[:10]:  # print first 10 lines only\n",
    "                print(f\"[{snippet['start']:.2f}s] {snippet['text']}\")\n",
    "\n",
    "elif choice == \"playlist\":\n",
    "    playlist_url = input(\"Enter the YouTube playlist URL: \").strip()\n",
    "    playlist = Playlist(playlist_url)\n",
    "    for video_url in playlist.video_urls:\n",
    "        transcripts = fetch_multilingual_transcripts(video_url)\n",
    "        if transcripts:\n",
    "            all_transcripts[video_url] = transcripts\n",
    "        time.sleep(5)\n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Enter 'video' or 'playlist'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d29fbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total transcript snippets prepared: 1104\n"
     ]
    }
   ],
   "source": [
    "docs_with_meta = []\n",
    "\n",
    "for video_url, lang_dict in all_transcripts.items():\n",
    "    # Extract video ID\n",
    "    parsed_url = urlparse(video_url)\n",
    "    video_id = parse_qs(parsed_url.query).get(\"v\", [\"\"])[0]\n",
    "\n",
    "    for lang, transcript_list in lang_dict.items():\n",
    "        for snippet in transcript_list:\n",
    "            metadata = {\n",
    "                \"video_id\": video_id,\n",
    "                \"language\": lang,\n",
    "                \"start_time\": snippet[\"start\"],\n",
    "                \"end_time\": snippet[\"start\"] + snippet[\"duration\"]\n",
    "            }\n",
    "            docs_with_meta.append({\n",
    "                \"text\": snippet[\"text\"].strip(),\n",
    "                \"metadata\": metadata\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4ceddd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(\n",
    "     chunk_size = 500, \n",
    "     chunk_overlap = 100, \n",
    "     separators=[\"\\n\\n\", \"\\n\", \".\", \"‡•§\", \"ÿü\", \"!\", \"„ÄÇ\", \"Ôºå\"] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44966f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "\n",
    "for doc in docs_with_meta:\n",
    "    split_docs = splitter.create_documents(\n",
    "        [doc[\"text\"]],\n",
    "        metadatas=[doc[\"metadata\"]]\n",
    "    )\n",
    "    chunks.extend(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "221a1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_chunks = [\n",
    "    c for c in chunks\n",
    "    if c.page_content.strip() and not c.page_content.startswith(\"---\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c141d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Unique cleaned chunks ready: 1103\n"
     ]
    }
   ],
   "source": [
    "unique_chunks = []\n",
    "seen_texts = set()\n",
    "\n",
    "for c in filtered_chunks:\n",
    "    text = c.page_content.strip()\n",
    "    if text and text not in seen_texts:\n",
    "        unique_chunks.append(c)\n",
    "        seen_texts.add(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f995b6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/distiluse-base-multilingual-cased-v2\", \n",
    "    model_kwargs={\"trust_remote_code\": True}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5e3abb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_for_chroma = [c.page_content for c in unique_chunks]\n",
    "metadatas_for_chroma = [c.metadata for c in unique_chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f62d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chroma vector store created successfully.\n"
     ]
    }
   ],
   "source": [
    "vector_store = Chroma.from_texts(\n",
    "    texts=texts_for_chroma,\n",
    "    embedding=embeddings,\n",
    "    metadatas=metadatas_for_chroma\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5874d5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(search_type= 'similarity', search_kwargs={'k':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a7813b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = input('Enter your query here : ')\n",
    "retrieved_docs = retriever.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "be16c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Rank 1 ---\n",
      "Score: -11.1153\n",
      "saana is a doctor philanthropist\n",
      "\n",
      "--- Rank 2 ---\n",
      "Score: -11.2455\n",
      "why did I want to be a doctor yes um\n",
      "\n",
      "--- Rank 3 ---\n",
      "Score: -11.1647\n",
      "up to my father who's an amazing doctor\n",
      "\n",
      "--- Rank 4 ---\n",
      "Score: -11.1469\n",
      "even in the video they show that the\n",
      "\n",
      "--- Rank 5 ---\n",
      "Score: -11.1549\n",
      "handsome and still he was a doctor not a\n"
     ]
    }
   ],
   "source": [
    "# Example: rerank top 5 retrieved documents\n",
    "scores = cross_encoder.predict([[query, d.page_content] for d in chunks[:5]])\n",
    "\n",
    "# 9Ô∏è‚É£ Sort documents by CrossEncoder scores descending\n",
    "reranked_docs = [doc for _, doc in sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)]\n",
    "\n",
    "# 10Ô∏è‚É£ Show reranked results\n",
    "for i, doc in enumerate(reranked_docs, 1):\n",
    "    print(f\"\\n--- Rank {i} ---\")\n",
    "    print(f\"Score: {scores[i-1]:.4f}\")\n",
    "    print(doc.page_content[:500])  # first 500 chars\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "043c911f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=['context_text', 'query'],\n",
    "    template='context_text:{context_text} \\n question:{query}'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c5c79095",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(retrieved_docs):\n",
    "    context_text = '\\n\\n'.join(doc.page_content for doc in reranked_docs)\n",
    "    return context_text\n",
    "\n",
    "context_text = format_docs(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7f385336",
   "metadata": {},
   "outputs": [],
   "source": [
    "parellel_chain = RunnableParallel({\n",
    "    'context_text' : retriever | RunnableLambda(format_docs),\n",
    "    'query' : RunnablePassthrough()\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7664ebbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  context_text: VectorStoreRetriever(tags=['Chroma', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x00000209DA328410>, search_kwargs={'k': 10})\n",
       "                | RunnableLambda(format_docs),\n",
       "  query: RunnablePassthrough()\n",
       "}\n",
       "| PromptTemplate(input_variables=['context_text', 'query'], input_types={}, partial_variables={}, template='context_text:{context_text} \\n question:{query}')\n",
       "| ChatOllama(model='llama3.1:latest', reasoning=False, base_url='http://localhost:11434')\n",
       "| StrOutputParser()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new = parellel_chain | prompt | llm | StrOutputParser()\n",
    "chain_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70326912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The term \"doctor philanthropist\" was not specifically mentioned as being discussed in the video. However, it is mentioned that saana\\'s father is an \"amazing doctor\", which implies that his profession and possibly some of his qualities or actions (implied by \"still he was a doctor\") might have had an impact on saana\\'s decision to become a doctor.'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_new.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f74696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (RAG_projects)",
   "language": "python",
   "name": "rag_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
